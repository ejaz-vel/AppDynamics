{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Basically, this is the order i have followed.\n",
    "Run your preprocessor.py to create processed data document vectors\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import linalg\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "dfFile = 'processedData/word_df.txt'\n",
    "docFile = 'processedData/documentVectors_train.txt'\n",
    "docFile_test = 'processedData/documentVectors_test.txt'\n",
    "#docFile = 'processedData/documentVectors.txt'\n",
    "\n",
    "# Count the number of lines in the DocumentFrequency(df) file\n",
    "def getNumUniqueWords():\n",
    "\tf = open(dfFile)\n",
    "\tcount = 0\n",
    "\tfor line in iter(f):\n",
    "\t\tcount += 1\n",
    "\tf.close()\n",
    "\treturn count\n",
    "\n",
    "# Count the number of lines in the DocumentVector(docVectors) file\n",
    "def getNumDocuments():\n",
    "\tf = open(docFile)\n",
    "\tcount = 0\n",
    "\tfor line in iter(f):\n",
    "\t\tcount += 1\n",
    "\tf.close()\n",
    "        print \"count \", count\n",
    "\treturn count\n",
    "\n",
    "def updateProbDisributionUsingSimilarity(documents, centroids):\n",
    "\tprobDistribution = []\n",
    "\trunningSum = 0.0\n",
    "\n",
    "\tfor n in range(len(documents)):\n",
    "\t\tmaxSim = 0.0000001\n",
    "\t\t# Find the cluster with whom the documents n has maximum similarity\n",
    "\t\tfor k in range(len(centroids)):\n",
    "\t\t\tsim = computeSimilarity(documents[n], centroids[k])\n",
    "\t\t\tif (sim.nnz == 0):\n",
    "\t\t\t\tsim = 0.0000001\n",
    "\t\t\telse:\n",
    "\t\t\t\tsim = sim.data[0]\n",
    "\n",
    "\t\t\tif maxSim < sim:\n",
    "\t\t\t\tmaxSim = sim\n",
    "\t\t# Take invserse of the maximum Similarity and update the probability distribution\n",
    "\t\t# This makes sure documents with lower similarity have higher probability of being sampled\n",
    "\t\tprobDistribution.append(1.0/maxSim)\n",
    "\t\trunningSum += 1.0/maxSim\n",
    "\n",
    "\tprobDistribution[:] = [x / runningSum for x in probDistribution]\n",
    "\treturn probDistribution\n",
    "\n",
    "# documents: The list of document vectors in the corpus\n",
    "# centroids: The list of centroids\n",
    "def updateProbDisribution(documents, centroids):\n",
    "\tprobDistribution = []\n",
    "\trunningSum = 0.0\n",
    "\tfor n in range(len(documents)):\n",
    "\t\tminDist = 9999999.0\n",
    "\t\t#Find the cluster at the closest distance from the document n\n",
    "\t\tfor k in range(len(centroids)):\n",
    "\t\t\tdiff = documents[n]-centroids[k]\n",
    "\t\t\tdist = diff.multiply(diff).sum()\n",
    "\t\t\tif dist < minDist:\n",
    "\t\t\t\tminDist = dist\n",
    "\t\t# Update the probability distribution so that documents with higher distance\n",
    "\t\t# have higher probability of being sampled.\n",
    "\t\tprobDistribution.append(minDist)\n",
    "\t\trunningSum += minDist\n",
    "\tprobDistribution[:] = [x / runningSum for x in probDistribution]\n",
    "\treturn probDistribution\n",
    "\n",
    "# This is the entry point for initializaing the centroids for KMeans++\n",
    "# documents: The list of document vectors in the corpus\n",
    "# numCentroid: Number of centroids that need to be initialized\n",
    "# useSimMetric: decides whether to use the distance metric or similarity metric\n",
    "def initCentroidKMeanspp(documents, numCentroid, useSimMetric):\n",
    "\tcentroids = []\n",
    "\t# Initially define the probability distribution giving equal probability of being sampled to each document.\n",
    "\tprobDistribution = [1.0/len(documents)] * len(documents)\n",
    "\tvalues = range(len(documents))\n",
    "\tfor k in range(numCentroid):\n",
    "\t\t# Sample a document from the distribution\n",
    "\t\tdocID = np.random.choice(values, 1, p=probDistribution)[0]\n",
    "\t\t# Add this document vector to the centroids list. In this case the centroids will be non-sparse.\n",
    "\t\tcentroids.append(documents[docID])\n",
    "\t\tprint(\"Initialized centroid: \" + str(k+1))\n",
    "\n",
    "\t\tif useSimMetric is True:\n",
    "\t\t\tprobDistribution = updateProbDisributionUsingSimilarity(documents, centroids)\n",
    "\t\telse:\n",
    "\t\t\tprobDistribution = updateProbDisribution(documents, centroids)\n",
    "\treturn centroids\n",
    "\n",
    "# This is the entry point for randomly initializaing the centroids for KMeans\n",
    "def randomInitCentroid(numCentroid, vectorSize):\n",
    "\tcentroids = []\n",
    "\tfor i in range(numCentroid):\n",
    "\t\t# Randomly Initialize a centroid. This will be a non-sparse vector\n",
    "\t\tcentroidVector = np.random.random(vectorSize)\n",
    "\t\t# Normalize the centroid Vector and add it to the list of centroids\n",
    "\t\tcentroids.append(centroidVector * 1/np.linalg.norm(centroidVector))\n",
    "\treturn centroids\n",
    "\n",
    "# Calculate the inverse Document Frequence of each term in the corpus.\n",
    "def getInverseDocumentFrequencyMap(numDocuments):\n",
    "\t# A map to store the IDF values for each term in the corpus\n",
    "\tdfMap = {}\n",
    "\tf = open(dfFile)\n",
    "\tfor line in iter(f):\n",
    "\t\tdf = line.split(\":\")\n",
    "\t\tdfMap[df[0]] = numDocuments / int(df[1])\n",
    "\treturn dfMap\n",
    "\n",
    "# Initializing the document vectors by reading the data from the input document vector file\n",
    "def getDocumentVectors(vectorSize):\n",
    "\tf = open(docFile)\n",
    "\tdocuments = []\n",
    "\tfor line in iter(f):\n",
    "\t\trow = []\n",
    "\t\tcol = []\n",
    "\t\tdata = []\n",
    "\t\twordFrequency = line.split()\n",
    "\t\tfor word in wordFrequency:\n",
    "\t\t\trow.append(0)\n",
    "\t\t\tcol.append(int(word.split(':')[0]))\n",
    "\t\t\t# Use only the term frequency to weigh each term in the document\n",
    "\t\t\tdata.append(int(word.split(':')[1]))\n",
    "\t\t# Create the sparse document vector using the CSR_matrix data structure\n",
    "\t\tspVector = csr_matrix((data, (row, col)), shape=(1, vectorSize))\n",
    "\t\t# Normalize the document vector and add the normalized vector to the list of document vectors\n",
    "\t\tdocuments.append(spVector.multiply(1/np.sqrt(spVector.multiply(spVector).sum())))\n",
    "\tf.close()\n",
    "\treturn documents\n",
    "\n",
    "# Use the TF-IDF weights to initialize the document vectors\n",
    "def getDocumentVectorsWithTFIDF(vectorSize,test=False):\n",
    "\tnumDocuments = getNumDocuments()\n",
    "\tidf = getInverseDocumentFrequencyMap(numDocuments)\n",
    "\tif(test):\n",
    "        f = open(docFile_test)\n",
    "    else:\n",
    "        f = open(docFile)\n",
    "\tdocuments = []\n",
    "\tfor line in iter(f):\n",
    "\t\trow = []\n",
    "\t\tcol = []\n",
    "\t\tdata = []\n",
    "\t\twordFrequency = line.split()\n",
    "\t\tfor word in wordFrequency:\n",
    "\t\t\trow.append(0)\n",
    "\t\t\tcol.append(int(word.split(':')[0]))\n",
    "\t\t\t# Use only the TF-IDF value to weigh each term in the document\n",
    "                        if( idf[word.split(':')[0]] <=0):\n",
    "                            print \"Negative or zero\", word, word.split(':'), idf[word.split(':')[0]]\n",
    "\t\t\tdata.append(int(word.split(':')[1]) * math.log(idf[word.split(':')[0]]))\n",
    "\t\t# Create the sparse document vector using the CSR_matrix data structure\n",
    "\t\tspVector = csr_matrix((data, (row, col)), shape=(1, vectorSize))\n",
    "\t\t# Normalize the document vector and add the normalized vector to the list of document vectors\n",
    "\t\tdocuments.append(spVector.multiply(1/np.sqrt(spVector.multiply(spVector).sum())))\n",
    "\tf.close()\n",
    "\treturn documents\n",
    "\n",
    "# Compute Similarity between the document and the given centroid\n",
    "def computeSimilarity(document, centroid):\n",
    "\t# Use the dot product as the similarity metric.\n",
    "\t# This is same as the cosine similarity because the document and centroid vectors have already been normalized before.\n",
    "\tsim = document.dot(centroid.T)[0]\n",
    "\treturn sim\n",
    "\n",
    "# Output the document-cluster assignment into the output file\n",
    "def printDocumentClusters(clusterAssignment, outputFileName):\n",
    "\tdocID = 1\n",
    "\toutput = open(outputFileName, \"w\")\n",
    "\tfor cluster in clusterAssignment:\n",
    "\t\tline = str(docID) + \" \" + str(cluster)\n",
    "\t\toutput.write(line)\n",
    "\t\toutput.write(\"\\n\")\n",
    "\t\tdocID += 1\n",
    "\toutput.close()\n",
    "\n",
    "# Update the centroid by calculating the arithmetic mean of all the documents assigned to the cluster\n",
    "def updateCentroid(documents, clusterAssignment, numClusters, vectorSize):\n",
    "\tcentroidSum = np.zeros(shape=(numClusters,vectorSize))\n",
    "\tcentroidCount = [0] * numClusters\n",
    "\n",
    "\tfor n in range(len(clusterAssignment)):\n",
    "\t\tcluster = clusterAssignment[n]\n",
    "\t\tcentroidSum[cluster] += documents[n]\n",
    "\t\tcentroidCount[cluster] += 1\n",
    "\n",
    "\tcentroids = []\n",
    "\tfor k in range(numClusters):\n",
    "\t\tif centroidCount[k] > 0:\n",
    "\t\t\t# Find the arithmetic mean\n",
    "\t\t\tcentroidVector = centroidSum[k] / centroidCount[k]\n",
    "\t\telse:\n",
    "\t\t\t# Empty Cluster! Randomly initialize the centroid in this case\n",
    "\t\t\tcentroidVector = np.random.random((1, vectorSize))\n",
    "\t\t# Normalize the centorid before updating the centroids list\n",
    "\t\tcentroids.append(centroidVector * 1/np.linalg.norm(centroidVector))\n",
    "\n",
    "\treturn centroids\n",
    "\n",
    "# Find the similarity of the documents with the cluster it is assigned to\n",
    "def computeClusteringScore(clusterAssignment, centroids, documents):\n",
    "\t#Calculate Average Similarity of each Cluster\n",
    "\tcentroidCount = [0] * len(centroids)\n",
    "\tcentroidSimilarityCount = [0] * len(centroids)\n",
    "\n",
    "\tfor n in range(len(clusterAssignment)):\n",
    "\t\tcluster = clusterAssignment[n]\n",
    "\t\tcentroidCount[cluster] += 1\n",
    "\t\tcentroidSimilarityCount[cluster] += computeSimilarity(documents[n], centroids[cluster])\n",
    "\n",
    "\ttotal = 0.0\n",
    "\tfor k in range(len(centroids)):\n",
    "\t\tif centroidCount[k] == 0:\n",
    "\t\t\tavgSimilarity = 0\n",
    "\t\telse:\n",
    "\t\t\tavgSimilarity = centroidSimilarityCount[k] / centroidCount[k]\n",
    "\t\ttotal += avgSimilarity\n",
    "\ttotal = total / len(centroids)\n",
    "\tprint \"Average Similarity of the clusters: \" + str(total)\n",
    "\treturn total\n",
    "\n",
    "# This is the main method that serves as the entry point to all the functionality\n",
    "# numClusters: The number of clusters to be learnt\n",
    "# useTFIDFWeight: This decides whether we need to use the TF-IDF weight or not while creating the document vector\n",
    "# useKMeanspp: This decides whether we need to use KMeans++ or not to initialize the clusters\n",
    "# useSimMetric: If we are using KMeans++, this parameter decides whether to use distance or similarity metric\n",
    "\n",
    "def findClusterAssignment(numClusters, useTFIDFWeight=False, useKMeanspp=False, useSimMetric=False):\n",
    "\tvectorSize = getNumUniqueWords()\n",
    "\tdocuments = []\n",
    "\tcentroids = []\n",
    "\n",
    "\tif useTFIDFWeight is True:\n",
    "\t\tprint(\"Using TF-IDF weights for Document Vectors\")\n",
    "\t\tdocuments = getDocumentVectorsWithTFIDF(vectorSize,False)\n",
    "\telse:\n",
    "\t\tprint(\"Using TF weights for Document Vectors\")\n",
    "\t\tdocuments = getDocumentVectors(vectorSize)\n",
    "\n",
    "\tif useKMeanspp == True:\n",
    "\t\tprint(\"Using KMeans++ to initialize centroids\")\n",
    "\t\tcentroids = initCentroidKMeanspp(documents, numClusters, useSimMetric)\n",
    "\telse:\n",
    "\t\tprint(\"Randomly initializing centroids\")\n",
    "\t\tcentroids = randomInitCentroid(numClusters, vectorSize)\n",
    "\n",
    "\tnumDocs = len(documents)\n",
    "\tpreviousClusterAssignment = [-1] * numDocs\n",
    "\tclusterAssignment = [0] * numDocs\n",
    "\n",
    "\tnumIterations = -1\n",
    "\tprint(\"Running Loyd's Algorithm for clustering\")\n",
    "\twhile ( np.array_equal(previousClusterAssignment,clusterAssignment) == False):\n",
    "\t\tprint(\"Iteration: \" + str(numIterations))\n",
    "\t\tpreviousClusterAssignment = clusterAssignment[:]\n",
    "\t\tfor n in range(numDocs):\n",
    "\t\t\tmaxSim = 0.0\n",
    "\t\t\tcluster = clusterAssignment[n]\n",
    "\t\t\tfor k in range(numClusters):\n",
    "\t\t\t\tsim = computeSimilarity(documents[n], centroids[k])\n",
    "\t\t\t\tif sim > maxSim:\n",
    "\t\t\t\t\tmaxSim = sim\n",
    "\t\t\t\t\tcluster = k\n",
    "\t\t\tclusterAssignment[n] = cluster\n",
    "\t\tnumIterations += 1\n",
    "\t\tcentroids = updateCentroid(documents, clusterAssignment, numClusters, vectorSize)\n",
    "\tsimilarity = computeClusteringScore(clusterAssignment, centroids, documents)\n",
    "\treturn clusterAssignment, centroids, similarity\n",
    "\n",
    "# New functions for cluster details and count and stats\n",
    "def getclustercount(centroids):\n",
    "    keys=range(len(centroids))\n",
    "    counts = dict.fromkeys(keys,0)\n",
    "    # print counts\n",
    "    f = open('output/assign.txt')\n",
    "    for line in iter(f):\n",
    "        cl =(line.strip().split(' ')[1].strip())\n",
    "        cl = int(cl)\n",
    "        if cl in counts:\n",
    "            counts[cl] +=1\n",
    "        else:\n",
    "            counts[cl] = 1\n",
    "    return counts\n",
    "\n",
    "#take the list of centroids and cretae tuples of their mean, std, number of points assigned to them\n",
    "def getclusterstats(centroids):\n",
    "    cms={}\n",
    "    counts = getclustercount(centroids)\n",
    "    for i in range(len(centroids)):\n",
    "    #     print \"Id\",i, \"mean\", np.mean(centroids[i]),\"std\",np.std(centroids[i])\n",
    "        cms[i]=(np.mean(centroids[i]),np.std(centroids[i]),counts[i])\n",
    "    return cms\n",
    "\n",
    "#takes in a vector t, centroid vector, mean, std,outliercount, threshold parameter hp2\n",
    "#and returns updated outlier count,and indication whether it s outlier\n",
    "def is_out(t,centroid,cm, std,oc,hp2):\n",
    "    outlier = False\n",
    "    l= zip(np.nonzero(t)[0],np.nonzero(t)[1])\n",
    "    for r,c in l:\n",
    "#         print t[r,c]\n",
    "        hih = (cm+ (hp2 * std) )\n",
    "        low = (cm-(hp2 * std))\n",
    "             \n",
    "        if (t[r,c] > hih):\n",
    "            oc+=1\n",
    "            outlier = True\n",
    "#             print \"hi\",hih, t[r,c]\n",
    "            break\n",
    "        if (t[r,c] < low):\n",
    "            oc+=1\n",
    "            outlier = True\n",
    "#             print \"lo\",low, t[r,c]\n",
    "            break\n",
    "#     print \"Escaped\"\n",
    "    return outlier, oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TF-IDF weights for Document Vectors\n",
      "count  15000\n",
      "found  1873 count 12928 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epi/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:159: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using KMeans++ to initialize centroids\n",
      "Initialized centroid: 1\n",
      "Initialized centroid: 2\n",
      "Initialized centroid: 3\n",
      "Initialized centroid: 4\n",
      "Initialized centroid: 5\n",
      "Initialized centroid: 6\n",
      "Initialized centroid: 7\n",
      "Initialized centroid: 8\n",
      "Initialized centroid: 9\n",
      "Initialized centroid: 10\n",
      "Running Loyd's Algorithm for clustering\n",
      "Iteration: -1\n",
      "Iteration: 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Average Similarity of the clusters: 0.491680165938\n"
     ]
    }
   ],
   "source": [
    "ca,centroids,simi = findClusterAssignment(10,True,True, True)\n",
    "printDocumentClusters(ca, 'output/assign.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 \t8\n",
      "4 \t8\n",
      "5 \t8\n"
     ]
    }
   ],
   "source": [
    "#hp2 is threshold parameter\n",
    "#hp is outlier density parameter\n",
    "#if you wnat to see hp vs numof clusters, just loop over hpl\n",
    "hpl = np.arange(0.25,2.5,0.25)\n",
    "# hpl = [0.8]\n",
    "# hp = 2.0\n",
    "hp2 = 3\n",
    "hp2l = np.arange(3,6,1)\n",
    "\n",
    "#tvs are vectors for streaming.\n",
    "vectorsize= getNumUniqueWords()\n",
    "tvs = getDocumentVectorsWithTFIDF(vectorsize,True)\n",
    "\n",
    "for hp in hpl:\n",
    "    #cms has mean std count for each centroid\n",
    "    cms =  getclusterstats(centroids)\n",
    "    #outlier count\n",
    "    out_count = 0\n",
    "    \n",
    "    #centroid vs outliercount near to them\n",
    "    out_dict = dict.fromkeys(range(len(centroids)),0)\n",
    "    \n",
    "    # number of evolving clusters\n",
    "    evol = 0\n",
    "    \n",
    "    #tvs are vectors for streaming.\n",
    "    for t in tvs:\n",
    "\n",
    "        mins = 0.0\n",
    "        for i in range(len(centroids)):\n",
    "            d = computeSimilarity(t,centroids[i])\n",
    "            if(d>=mins):\n",
    "                mins= d\n",
    "                cl=i\n",
    "    #     print \"closest cluster :\", cl\n",
    "        clu = centroids[cl]\n",
    "        cm = cms[i][0]\n",
    "        cstd = cms[i][1]\n",
    "        outlier, out_count = is_out(t, clu, cm, cstd, out_count,hp2)\n",
    "        \n",
    "        if(not outlier):\n",
    "    #         assign , basically increment count for that centroid\n",
    "            ccc = cms[i][2]\n",
    "            cms[i] = (cms[i][0],cms[i][1],ccc+1)\n",
    "    #         print \"i reached here\",cms[i]\n",
    "        else:\n",
    "    #         print \"outlier with respect to \",cl\n",
    "            out_dict[cl]+=1\n",
    "            if out_count >= (hp * cms[i][2]):\n",
    "    #             print \"---------\\ntrigger re-cluster\"\n",
    "                evol+=1\n",
    "    #             print \"Number of evolved clusters\", evol\n",
    "    #             print \"out_count\", out_count, \"cluster with max outlier proximity\", max(out_dict, key=out_dict.get)\n",
    "    #             print \"----------re-cluster done-------\"\n",
    "                out_count= 0\n",
    "                out_dict = dict.fromkeys(range(len(centroids)),0)\n",
    "    print \"Denisty\",hp,\"Number of clusters evolved\", evol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
